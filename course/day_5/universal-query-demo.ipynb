{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Universal Query for Hybrid Retrieval\n",
    "\n",
    "In this hands-on demo, we'll build a research paper discovery system using real arXiv data and Qdrant's Universal Query API. You'll see how to:\n",
    "\n",
    "- Fetch real research papers from arXiv\n",
    "- Generate **dense**, **sparse**, and **ColBERT** embeddings\n",
    "- Execute hybrid retrieval with intelligent filtering\n",
    "- Combine multiple search strategies in a single query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Dependencies\n",
    "\n",
    "Install required packages for working with Qdrant, embeddings, and arXiv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.2\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install -q \"qdrant-client~=1.15.1\" \"fastembed~=0.7.3\" arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "# Initialize Qdrant client\n",
    "client = QdrantClient(\n",
    "    url=\"https://your-cluster-url.cloud.qdrant.io\",\n",
    "    api_key=\"your-api-key\",\n",
    ")\n",
    "\n",
    "collection_name = \"research-papers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create the Collection\n",
    "\n",
    "Configure our collection with three vector types for multi-stage retrieval:\n",
    "- **Dense vectors** (384-dim) for semantic understanding\n",
    "- **Sparse vectors** for exact keyword matching\n",
    "- **ColBERT multivectors** (128-dim) for fine-grained reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Collection 'research-papers' created\n"
     ]
    }
   ],
   "source": [
    "# Clean state\n",
    "if client.collection_exists(collection_name=collection_name):\n",
    "    client.delete_collection(collection_name=collection_name)\n",
    "\n",
    "# Create collection with three vector types\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config={\n",
    "        \"dense\": models.VectorParams(size=384, distance=models.Distance.COSINE),\n",
    "        \"colbert\": models.VectorParams(\n",
    "            size=128,\n",
    "            distance=models.Distance.COSINE,\n",
    "            multivector_config=models.MultiVectorConfig(\n",
    "                comparator=models.MultiVectorComparator.MAX_SIM\n",
    "            ),\n",
    "        ),\n",
    "    },\n",
    "    sparse_vectors_config={\n",
    "        \"sparse\": models.SparseVectorParams(\n",
    "            index=models.SparseIndexParams(on_disk=False)\n",
    "        )\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"✓ Collection '{collection_name}' created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Payload Indexes\n",
    "\n",
    "Create indexes for the fields we'll filter by. Qdrant applies filters at the HNSW search level, not as post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Payload indexes created\n"
     ]
    }
   ],
   "source": [
    "# Index fields for efficient filtering\n",
    "client.create_payload_index(\n",
    "    collection_name=collection_name,\n",
    "    field_name=\"research_area\",\n",
    "    field_schema=\"keyword\",\n",
    ")\n",
    "client.create_payload_index(\n",
    "    collection_name=collection_name,\n",
    "    field_name=\"open_access\",\n",
    "    field_schema=\"bool\",\n",
    ")\n",
    "client.create_payload_index(\n",
    "    collection_name=collection_name,\n",
    "    field_name=\"published_date\",\n",
    "    field_schema=\"datetime\",\n",
    ")\n",
    "client.create_payload_index(\n",
    "    collection_name=collection_name,\n",
    "    field_name=\"impact_score\",\n",
    "    field_schema=\"float\",\n",
    ")\n",
    "client.create_payload_index(\n",
    "    collection_name=collection_name,\n",
    "    field_name=\"citation_count\",\n",
    "    field_schema=\"integer\",\n",
    ")\n",
    "\n",
    "print(\"✓ Payload indexes created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Embedding Models\n",
    "\n",
    "Load FastEmbed models for generating all three embedding types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding models...\n",
      "✓ All embedding models loaded\n"
     ]
    }
   ],
   "source": [
    "from fastembed import (\n",
    "    TextEmbedding, \n",
    "    SparseTextEmbedding, \n",
    "    LateInteractionTextEmbedding\n",
    ")\n",
    "\n",
    "DENSE_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"  # 384-dim\n",
    "SPARSE_MODEL_ID = \"prithivida/Splade_PP_en_v1\"  # SPLADE sparse\n",
    "COLBERT_MODEL_ID = \"colbert-ir/colbertv2.0\"  # 128-dim multivector\n",
    "\n",
    "print(\"Loading embedding models...\")\n",
    "dense_model = TextEmbedding(DENSE_MODEL_ID)\n",
    "sparse_model = SparseTextEmbedding(SPARSE_MODEL_ID)\n",
    "colbert_model = LateInteractionTextEmbedding(COLBERT_MODEL_ID)\n",
    "\n",
    "print(\"✓ All embedding models loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Fetch Papers from arXiv\n",
    "\n",
    "Let's search arXiv for papers about transformers and multimodal learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching papers from arXiv...\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "\n",
    "# Initialize arXiv client and search\n",
    "arxiv_client = arxiv.Client()\n",
    "\n",
    "search = arxiv.Search(\n",
    "    query=\"transformer AND multimodal\",\n",
    "    max_results=50,\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    ")\n",
    "\n",
    "print(\"Fetching papers from arXiv...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Process and Ingest Papers\n",
    "\n",
    "For each paper, we'll:\n",
    "1. Extract metadata (title, authors, abstract, date)\n",
    "2. Generate dense, sparse, and ColBERT embeddings\n",
    "3. Upload to Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing paper 1...\n",
      "Processing paper 11...\n",
      "Processing paper 21...\n",
      "Processing paper 31...\n",
      "Processing paper 41...\n",
      "\n",
      "✓ Processed 50 papers\n"
     ]
    }
   ],
   "source": [
    "points = []\n",
    "\n",
    "for i, paper in enumerate(arxiv_client.results(search)):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processing paper {i + 1}...\")\n",
    "\n",
    "    # Extract paper information\n",
    "    abstract = paper.summary\n",
    "\n",
    "    # Generate embeddings for the abstract\n",
    "    dense_vector = next(dense_model.embed(abstract))\n",
    "    sparse_vector = next(sparse_model.embed(abstract)).as_object()\n",
    "    colbert_vector = next(colbert_model.embed(abstract))\n",
    "\n",
    "    # Determine research area (simplified categorization)\n",
    "    research_area = \"machine_learning\"\n",
    "    if any(term in abstract.lower() for term in [\"vision\", \"image\", \"visual\"]):\n",
    "        research_area = \"computer_vision\"\n",
    "    elif any(term in abstract.lower() for term in [\"language\", \"nlp\", \"text\"]):\n",
    "        research_area = \"nlp\"\n",
    "\n",
    "    # Create point\n",
    "    point = models.PointStruct(\n",
    "        id=i,\n",
    "        payload={\n",
    "            \"title\": paper.title,\n",
    "            \"authors\": [author.name for author in paper.authors],\n",
    "            \"abstract\": abstract,\n",
    "            \"published_date\": paper.published.isoformat(),\n",
    "            \"research_area\": research_area,\n",
    "            \"citation_count\": 10,  # Placeholder (would need external API)\n",
    "            \"impact_score\": 0.7,  # Placeholder (would need calculation)\n",
    "            \"open_access\": True,  # arXiv is open access\n",
    "            \"arxiv_id\": paper.entry_id,\n",
    "        },\n",
    "        vector={\n",
    "            \"dense\": dense_vector,\n",
    "            \"sparse\": sparse_vector,\n",
    "            \"colbert\": colbert_vector,\n",
    "        },\n",
    "    )\n",
    "    points.append(point)\n",
    "\n",
    "print(f\"\\n✓ Processed {len(points)} papers\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Since we use FastEmbed, we could also create the points with alternative syntax using so-called local inference. Here's how that would look:\n",
    "\n",
    "```python\n",
    "point = models.PointStruct(\n",
    "\tid=i,\n",
    "\tpayload={\n",
    "\t\t...  # same as above\n",
    "\t},\n",
    "\tvector={\n",
    "\t\t\"dense\": model.Document(\n",
    "\t\t\tmodel=DENSE_MODEL_ID,\n",
    "\t\t\ttext=abstract,\n",
    "\t\t),\n",
    "\t\t\"sparse\": model.Document(\n",
    "\t\t\tmodel=SPARSE_MODEL_ID,\n",
    "\t\t\ttext=abstract,\n",
    "\t\t),\n",
    "\t\t\"colbert\": model.Document(\n",
    "\t\t\tmodel=COLBERT_MODEL_ID,\n",
    "\t\t\ttext=abstract,\n",
    "\t\t),\n",
    "\t},\n",
    ")\n",
    "```\n",
    "\n",
    "FastEmbed would then handle embedding generation within Qdrant during upload. This syntax is also compatible with [Cloud Inference](https://qdrant.tech/documentation/cloud/inference/) if you prefer to offload embedding generation to Qdrant Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Uploaded 50 research papers to Qdrant\n"
     ]
    }
   ],
   "source": [
    "# Upload to Qdrant\n",
    "client.upload_points(\n",
    "    collection_name=collection_name,\n",
    "    points=points,\n",
    ")\n",
    "\n",
    "print(f\"✓ Uploaded {len(points)} research papers to Qdrant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Execute the Universal Query\n",
    "\n",
    "Now we'll search for papers using hybrid retrieval that combines:\n",
    "1. **Parallel dense and sparse search**\n",
    "2. **Reciprocal Rank Fusion (RRF)**\n",
    "3. **ColBERT reranking**\n",
    "4. **Global filtering** (applied at every stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our research query\n",
    "research_query = \"transformer architectures for multimodal learning\"\n",
    "\n",
    "# Encode query with all three models\n",
    "research_query_dense = next(dense_model.query_embed(research_query))\n",
    "research_query_sparse = next(sparse_model.query_embed(research_query)).as_object()\n",
    "research_query_colbert = next(colbert_model.query_embed(research_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Global Filter\n",
    "\n",
    "This filter will automatically propagate to all prefetch stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define quality constraints\n",
    "global_filter = models.Filter(\n",
    "    must=[\n",
    "        # Research domain filtering\n",
    "        models.FieldCondition(\n",
    "            key=\"research_area\",\n",
    "            match=models.MatchAny(\n",
    "                any=[\n",
    "                    \"machine_learning\",\n",
    "                    \"computer_vision\",\n",
    "                    \"nlp\",\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        # Open access only\n",
    "        models.FieldCondition(key=\"open_access\", match=models.MatchValue(value=True)),\n",
    "        # Recent research (last 6 years)\n",
    "        models.FieldCondition(\n",
    "            key=\"published_date\",\n",
    "            range=models.DatetimeRange(\n",
    "                gte=(datetime.now() - timedelta(days=365 * 6)).isoformat()\n",
    "            ),\n",
    "        ),\n",
    "        # High-impact papers\n",
    "        models.FieldCondition(key=\"impact_score\", range=models.Range(gte=0.6)),\n",
    "        # Well-cited work\n",
    "        models.FieldCondition(key=\"citation_count\", range=models.Range(gte=5)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Execute Multi-Stage Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Query executed successfully\n",
      "Found 10 results\n"
     ]
    }
   ],
   "source": [
    "# Stage 1: Parallel prefetch (dense + sparse)\n",
    "hybrid_query = [\n",
    "    models.Prefetch(query=research_query_dense, using=\"dense\", limit=100),\n",
    "    models.Prefetch(query=research_query_sparse, using=\"sparse\", limit=100),\n",
    "]\n",
    "\n",
    "# Stage 2: Fusion with RRF\n",
    "fusion_query = models.Prefetch(\n",
    "    prefetch=hybrid_query,\n",
    "    query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
    "    limit=100,\n",
    ")\n",
    "\n",
    "# Stage 3: Execute with ColBERT reranking\n",
    "response = client.query_points(\n",
    "    collection_name=collection_name,\n",
    "    prefetch=fusion_query,\n",
    "    query=research_query_colbert,\n",
    "    using=\"colbert\",\n",
    "    query_filter=global_filter,  # Automatically propagates to all stages\n",
    "    limit=10,\n",
    "    with_payload=True,\n",
    ")\n",
    "\n",
    "print(f\"✓ Query executed successfully\")\n",
    "print(f\"Found {len(response.points)} results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Let's examine the top papers discovered by our hybrid retrieval system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "TOP RESEARCH PAPERS\n",
      "====================================================================================================\n",
      "\n",
      "1. Collaborative Text-to-Image Generation via Multi-Agent Reinforcement Learning and Semantic Fusion\n",
      "   Authors: Jiabao Shi, Minfeng Qi, Lefeng Zhang...\n",
      "   Published: 2025-10-12\n",
      "   Research Area: computer_vision\n",
      "   Relevance Score: 25.1145\n",
      "   arXiv: http://arxiv.org/abs/2510.10633v1\n",
      "   Abstract: Multimodal text-to-image generation remains constrained by the difficulty of\n",
      "maintaining semantic alignment and professional-level detail across diverse\n",
      "visual domains. We propose a multi-agent reinfo...\n",
      "\n",
      "2. Beyond Appearance: Transformer-based Person Identification from Conversational Dynamics\n",
      "   Authors: Masoumeh Chapariniya, Teodora Vukovic, Sarah Ebling...\n",
      "   Published: 2025-10-06\n",
      "   Research Area: machine_learning\n",
      "   Relevance Score: 24.5158\n",
      "   arXiv: http://arxiv.org/abs/2510.04753v1\n",
      "   Abstract: This paper investigates the performance of transformer-based architectures\n",
      "for person identification in natural, face-to-face conversation scenario. We\n",
      "implement and evaluate a two-stream framework th...\n",
      "\n",
      "3. CAT: Curvature-Adaptive Transformers for Geometry-Aware Learning\n",
      "   Authors: Ryan Y. Lin, Siddhartha Ojha, Nicholas Bai\n",
      "   Published: 2025-10-02\n",
      "   Research Area: computer_vision\n",
      "   Relevance Score: 22.4639\n",
      "   arXiv: http://arxiv.org/abs/2510.01634v1\n",
      "   Abstract: Transformers achieve strong performance across diverse domains but implicitly\n",
      "assume Euclidean geometry in their attention mechanisms, limiting their\n",
      "effectiveness on data with non-Euclidean structure...\n",
      "\n",
      "4. BitMar: Low-Bit Multimodal Fusion with Episodic Memory for Edge Devices\n",
      "   Authors: Euhid Aman, Esteban Carlin, Hsing-Kuo Pao...\n",
      "   Published: 2025-10-12\n",
      "   Research Area: computer_vision\n",
      "   Relevance Score: 22.2900\n",
      "   arXiv: http://arxiv.org/abs/2510.10560v1\n",
      "   Abstract: Cross-attention transformers and other multimodal vision-language models\n",
      "excel at grounding and generation; however, their extensive, full-precision\n",
      "backbones make it challenging to deploy them on edg...\n",
      "\n",
      "5. Complementary and Contrastive Learning for Audio-Visual Segmentation\n",
      "   Authors: Sitong Gong, Yunzhi Zhuge, Lu Zhang...\n",
      "   Published: 2025-10-11\n",
      "   Research Area: computer_vision\n",
      "   Relevance Score: 22.2407\n",
      "   arXiv: http://arxiv.org/abs/2510.10051v1\n",
      "   Abstract: Audio-Visual Segmentation (AVS) aims to generate pixel-wise segmentation maps\n",
      "that correlate with the auditory signals of objects. This field has seen\n",
      "significant progress with numerous CNN and Transf...\n",
      "\n",
      "6. BioAutoML-NAS: An End-to-End AutoML Framework for Multimodal Insect Classification via Neural Architecture Search on Large-Scale Biodiversity Data\n",
      "   Authors: Arefin Ittesafun Abian, Debopom Sutradhar, Md Rafi Ur Rashid...\n",
      "   Published: 2025-10-07\n",
      "   Research Area: computer_vision\n",
      "   Relevance Score: 20.8009\n",
      "   arXiv: http://arxiv.org/abs/2510.05888v1\n",
      "   Abstract: Insect classification is important for agricultural management and ecological\n",
      "research, as it directly affects crop health and production. However, this task\n",
      "remains challenging due to the complex cha...\n",
      "\n",
      "7. Towards fairer public transit: Real-time tensor-based multimodal fare evasion and fraud detection\n",
      "   Authors: Peter Wauyo, Dalia Bwiza, Alain Murara...\n",
      "   Published: 2025-10-02\n",
      "   Research Area: computer_vision\n",
      "   Relevance Score: 20.6940\n",
      "   arXiv: http://arxiv.org/abs/2510.02165v1\n",
      "   Abstract: This research introduces a multimodal system designed to detect fraud and\n",
      "fare evasion in public transportation by analyzing closed circuit television\n",
      "(CCTV) and audio data. The proposed solution uses...\n",
      "\n",
      "8. Provable Speech Attributes Conversion via Latent Independence\n",
      "   Authors: Jonathan Svirsky, Ofir Lindenbaum, Uri Shaham\n",
      "   Published: 2025-10-06\n",
      "   Research Area: computer_vision\n",
      "   Relevance Score: 20.4618\n",
      "   arXiv: http://arxiv.org/abs/2510.05191v2\n",
      "   Abstract: While signal conversion and disentangled representation learning have shown\n",
      "promise for manipulating data attributes across domains such as audio, image,\n",
      "and multimodal generation, existing approaches...\n",
      "\n",
      "9. A Spatial-Spectral-Frequency Interactive Network for Multimodal Remote Sensing Classification\n",
      "   Authors: Hao Liu, Yunhao Gao, Wei Li...\n",
      "   Published: 2025-10-06\n",
      "   Research Area: computer_vision\n",
      "   Relevance Score: 20.4072\n",
      "   arXiv: http://arxiv.org/abs/2510.04628v1\n",
      "   Abstract: Deep learning-based methods have achieved significant success in remote\n",
      "sensing Earth observation data analysis. Numerous feature fusion techniques\n",
      "address multimodal remote sensing image classificati...\n",
      "\n",
      "10. Growing Visual Generative Capacity for Pre-Trained MLLMs\n",
      "   Authors: Hanyu Wang, Jiaming Han, Ziyan Yang...\n",
      "   Published: 2025-10-02\n",
      "   Research Area: computer_vision\n",
      "   Relevance Score: 20.3669\n",
      "   arXiv: http://arxiv.org/abs/2510.01546v1\n",
      "   Abstract: Multimodal large language models (MLLMs) extend the success of language\n",
      "models to visual understanding, and recent efforts have sought to build unified\n",
      "MLLMs that support both understanding and genera...\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 100)\n",
    "print(\"TOP RESEARCH PAPERS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, hit in enumerate(response.points or [], 1):\n",
    "    paper = hit.payload\n",
    "    print(f\"\\n{i}. {paper['title']}\")\n",
    "    print(\n",
    "        f\"   Authors: {', '.join(paper['authors'][:3])}{'...' if len(paper['authors']) > 3 else ''}\"\n",
    "    )\n",
    "    print(f\"   Published: {paper['published_date'][:10]}\")\n",
    "    print(f\"   Research Area: {paper['research_area']}\")\n",
    "    print(f\"   Relevance Score: {hit.score:.4f}\")\n",
    "    print(f\"   arXiv: {paper['arxiv_id']}\")\n",
    "    print(f\"   Abstract: {paper['abstract'][:200]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
