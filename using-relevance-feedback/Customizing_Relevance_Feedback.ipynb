{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Relevance Feedback in Qdrant\n",
        "\n",
        "This notebook accompanies the tutorial at https://qdrant.tech/documentation/tutorials-search-engineering/using-relevance-feedback/"
      ],
      "metadata": {
        "id": "v7Zkdiv4RfJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Qdrant 1.17 we introduced a new [Relevance Feedback Query](https://qdrant.tech/documentation/concepts/search-relevance/#relevance-feedback), our scalable, first ever vector index-native approach to [incorporating relevance feedback](https://qdrant.tech/articles/search-feedback-loop/) in retrieval.\n",
        "\n",
        "In this tutorial, you'll see how to customize the Relevance Feedback Query for your Qdrant collection and feedback model, add it to your search pipeline, and evaluate the gains it brings."
      ],
      "metadata": {
        "id": "Xv5GNd4QRvfv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Relevance Feedback\n",
        "\n",
        "*Relevance feedback distills signals about the relevance of current search results into the next retrieval iteration, surfacing better results over time.*\n",
        "\n",
        "The Relevance Feedback Query uses a small amount of model-generated feedback on the search results to guide the retriever through the entire vector space on the next retrieval iteration, nudging search toward more relevant results. A detailed description of how it works can be found in the article [Relevance Feedback in Qdrant](https://qdrant.tech/articles/relevance-feedback/).\n",
        "\n",
        "### Relevance Feedback Strategy\n",
        "\n",
        "To use the feedback for guiding a retriever in the vector space, there are several possible strategies. For now, only the naive strategy is available -- [a simple 3-parameter formula](https://qdrant.tech/documentation/concepts/search-relevance/#naive-strategy) which adjusts similarity scoring based on the feedback.\n",
        "\n",
        "For the strategy to work well, the parameters of this naive formula should be customized for your data, retriever and feedback model. For convenience, we provide you with a [`qdrant-relevance-feedback` Python package](https://pypi.org/project/qdrant-relevance-feedback/) that gives you the corresponding parameters for your use case.\n",
        "\n",
        "This tutorial will demonstrate how to use it together with the Relevance Feedback Query API."
      ],
      "metadata": {
        "id": "1BmMcS91R_pK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "Install the aforementioned package (it will automatically install the Qdrant Client as its dependency):"
      ],
      "metadata": {
        "id": "UtrFU5y9SmzH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhXf89rz4uZ-"
      },
      "outputs": [],
      "source": [
        "!pip -q install qdrant-relevance-feedback fastembed"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to use a [Qdrant Cloud Free Tier Cluster](https://qdrant.tech/documentation/cloud/create-cluster/#free-clusters) (make sure it's on version 1.17.0+) and Free Embedding Inference available on this Free Tier Cluster.\n",
        "\n",
        "[Create a Qdrant Free Tier Cluster](https://cloud.qdrant.io/) and initialize the Qdrant Client:"
      ],
      "metadata": {
        "id": "WTD5sF99SvHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from qdrant_client import QdrantClient\n",
        "from google.colab import userdata\n",
        "\n",
        "client = QdrantClient(\n",
        "    url=userdata.get('QDRANT_URL'),\n",
        "    api_key=userdata.get('QDRANT_API_KEY'),\n",
        "    cloud_inference=True #to be able to use free embedding inference on Qdrant Cloud, for convenience & to reduce latency\n",
        ")"
      ],
      "metadata": {
        "id": "WHrKQ2-h_jfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset\n",
        "\n",
        "We'll use one of the datasets available in the Qdrant Cluster UI.\n",
        "\n",
        "1. Go to Cluster UI -> Datasets.\n",
        "1. Out of 3 datasets there, pick the first one, “Qdrant Web Documentation”. It consists of small text chunks from our [documentation website](https://qdrant.tech/documentation/).\n",
        "1. Press the Import button and type in a collection name -- documentation.\n",
        "\n",
        "After a couple of seconds you should see a green status pop up “Snapshot successfully imported” and a collection named documentation in the list of all collections.\n",
        "\n",
        "Now we have a collection to work with and optimize semantic search in."
      ],
      "metadata": {
        "id": "tXK5X7foTDE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "COLLECTION_NAME = \"documentation\""
      ],
      "metadata": {
        "id": "dhP87mlHFBGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retriever\n",
        "\n",
        "The retriever is an embedding model that converts your raw data into vectors for semantic similarity search. The Relevance Feedback Query will help you optimize your retriever's ability to find relevant results in your data.\n",
        "\n",
        "Our retriever is already defined by our collection -- as the `all-MiniLM-L6-v2` tag on the dataset's “VECTORS CONFIG” shows us, vectors in the collection are produced with the `all-MiniLM-L6-v2` model.\n",
        "\n",
        "Let's define it in our qdrant-relevance-feedback framework."
      ],
      "metadata": {
        "id": "JFqGTMnLTan2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from qdrant_relevance_feedback.retriever import QdrantRetriever\n",
        "\n",
        "retriever = QdrantRetriever(\"sentence-transformers/all-minilm-l6-v2\")"
      ],
      "metadata": {
        "id": "tAFZx_wEFB28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*QdrantRetriever supports all Qdrant Cloud Inference and FastEmbed models, and `all-MiniLM-L6-v2` is covered by Qdrant Free Tier Cloud Inference. You can define and provide your custom retrievers, too.*\n",
        "\n",
        "Our documentation collection has only one vector per point -- Default vector. However, in Qdrant, one can have several named vectors per point.\n",
        "\n",
        "We need to provide the name of the vector associated with the retriever that we're planning to optimize with feedback.\n",
        "\n"
      ],
      "metadata": {
        "id": "jLwr-VizT0Wn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RETRIEVER_VECTOR_NAME = None # None if it's a default vector or your named vector handle in Qdrant's collection"
      ],
      "metadata": {
        "id": "BYXeA1aHT3HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to point our framework to the raw data which is vectorized with our retriever model. Here, the raw data is the `text` field in the point's [payload](https://qdrant.tech/documentation/concepts/payload/#payload). Simply put, we search on text snippets -- in their vectorized form."
      ],
      "metadata": {
        "id": "Yz22-3VST8j0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PAYLOAD_KEY = \"text\""
      ],
      "metadata": {
        "id": "5ecdHlsjUEy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your raw data is stored elsewhere outside of Qdrant, you can redefine the `payload_retrieval` part in `qdrant-relevance-feedback`."
      ],
      "metadata": {
        "id": "Lo6Mc8AoUJz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feedback Model\n",
        "\n",
        "The **feedback model** looks at the results your retriever provides and scores how relevant they are. The retriever then uses this feedback, via the Relevance Feedback Query interface, to orient itself better in the vector space and surface more relevant documents.\n",
        "\n",
        "As a provider of a feedback model in this tutorial, we'll use Qdrant's lightweight inference library called [FastEmbed](https://qdrant.tech/documentation/fastembed/).\n",
        "\n",
        "As `all-MiniLM-L6-v2` is a small (only 384 dimensions) and weak model, we don't have to pick a feedback model which is too strong or expensive. Such a small retriever won't be receptive to sophisticated feedback anyway.\n",
        "\n",
        "Hence, for simplicity, we can use a bi-encoder of a bigger dimensionality, for example, `mxbai-embed-large-v1` of 1024 dimensions."
      ],
      "metadata": {
        "id": "MIxtyYPhUNjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from qdrant_relevance_feedback.feedback import FastembedFeedback\n",
        "\n",
        "feedback = FastembedFeedback(\"mixedbread-ai/mxbai-embed-large-v1\")"
      ],
      "metadata": {
        "id": "9fssOQczFMIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code will download `mxbai-embed-large-v1` for local inference.\n",
        "\n",
        "*We provide FastembedFeedback with all the FastEmbed models. However, you can define and provide your custom feedback model, too. It can be anything: bi-encoder, late interaction model, cross-encoder or LLM.*"
      ],
      "metadata": {
        "id": "tMzDC_n3Us37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Customizing the Collection, Retriever, and Feedback Model\n",
        "\n",
        "Now we have everything in place: retriever, collection, feedback model."
      ],
      "metadata": {
        "id": "ZRu9X1ZyVHkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from qdrant_relevance_feedback import RelevanceFeedback\n",
        "\n",
        "relevance_feedback = RelevanceFeedback(\n",
        "        retriever=retriever,\n",
        "        feedback=feedback,\n",
        "        client=client,\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        vector_name=RETRIEVER_VECTOR_NAME,\n",
        "        payload_key=PAYLOAD_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "hiFvC33MFoSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can run a small training process, which will collect data and adjust the Relevance Feedback Query parameters (naive strategy) for this triplet.\n",
        "\n",
        "###Training Data\n",
        "\n",
        "How well the relevance feedback formula fits your data depends on the amount and quality of the training data (how realistic it is for your use case). However, in general, we need **a very little amount** of it (50-300 queries will suffice), as we only need to train 3 parameters.\n",
        "\n",
        "We've generated 50 queries with Claude Code, using the prompt *“Generate 50 short search queries (3-10 words each) that a developer might type when browsing Qdrant's website.”.*"
      ],
      "metadata": {
        "id": "O8G93vHzWgye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_queries = [\n",
        "      \"qdrant filtering nested object payload\",\n",
        "      \"vector database for fraud detection\",\n",
        "      \"qdrant golang client example\",\n",
        "      \"sparse dense fusion ranking qdrant\",\n",
        "      \"qdrant write ahead log explained\",\n",
        "      \"sentence transformers qdrant integration\",\n",
        "      \"qdrant ef_construct m parameter tuning\",\n",
        "      \"music recommendation engine vector search\",\n",
        "      \"qdrant on kubernetes helm chart\",\n",
        "      \"qdrant payload index types keyword integer\",\n",
        "      \"qdrant segment optimizer internals\",\n",
        "      \"content moderation with embeddings\",\n",
        "      \"qdrant point struct fields\",\n",
        "      \"zero shot classification vector database\",\n",
        "      \"qdrant telemetry prometheus grafana\",\n",
        "      \"document deduplication vector similarity\",\n",
        "      \"qdrant replication factor setup\",\n",
        "      \"llama index qdrant integration\",\n",
        "      \"qdrant single node vs distributed\",\n",
        "      \"news article clustering embeddings\",\n",
        "      \"qdrant read consistency levels\",\n",
        "      \"face recognition vector search\",\n",
        "      \"qdrant timeout configuration tuning\",\n",
        "      \"langchain qdrant vector store\",\n",
        "      \"qdrant exact search brute force\",\n",
        "      \"job matching semantic search qdrant\",\n",
        "      \"qdrant shard transfer rebalancing\",\n",
        "      \"openai embeddings qdrant tutorial\",\n",
        "      \"qdrant versioning API changelog\",\n",
        "      \"patent search vector database use case\",\n",
        "      \"qdrant cohere embeddings example\",\n",
        "      \"customer support ticket routing qdrant\",\n",
        "      \"qdrant memory mapped files\",\n",
        "      \"multilingual search qdrant embeddings\",\n",
        "      \"qdrant indexing speed optimization\",\n",
        "      \"code search semantic embeddings qdrant\",\n",
        "      \"qdrant terraform cloud infrastructure\",\n",
        "      \"reducing hallucinations LLM qdrant grounding\",\n",
        "      \"qdrant aws deployment guide\",\n",
        "      \"long document search chunking qdrant\",\n",
        "      \"qdrant community discord forum\",\n",
        "      \"e-learning personalization vector search\",\n",
        "      \"qdrant concurrent requests handling\",\n",
        "      \"graph neural networks vector store\",\n",
        "      \"qdrant enterprise security features\",\n",
        "      \"product catalog search qdrant retail\",\n",
        "      \"qdrant contributing open source guide\",\n",
        "      \"time series anomaly vector embeddings\",\n",
        "      \"qdrant gRPC vs REST performance\",\n",
        "      \"qdrant vector search instagram feed ranking\",\n",
        "]"
      ],
      "metadata": {
        "id": "LnQZkgi2H_-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another parameter controlling training data size is how many responses we retrieve from our collection per each query."
      ],
      "metadata": {
        "id": "Nbo5XiZHWy_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_LIMIT = 25"
      ],
      "metadata": {
        "id": "J7WfIXJrWtYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bigger the `TRAIN_LIMIT`, the more training data our formula gets, but the more expensive and slow the training becomes.\n",
        "\n",
        "For training, we need our feedback model to provide ground truth relevancy scores, so it rescores #queries * `TRAIN_LIMIT`, here 25 * 50 = 1250 query-document pairs. Adjust based on your training budget."
      ],
      "metadata": {
        "id": "dw_zyxhfW1Jb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Customization\n",
        "\n",
        "Now we can run the training:"
      ],
      "metadata": {
        "id": "7hExXY8aW7m3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "formula_params = relevance_feedback.train(\n",
        "    queries=train_queries,\n",
        "    limit=TRAIN_LIMIT\n",
        ")"
      ],
      "metadata": {
        "id": "moDGJxdLWuVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll see a “Building training data” process running on 50 queries. Additionally, the framework will provide you with a sensibility check, something like:\n",
        "\n",
        "```\n",
        "On 22.00% of training queries the feedback model strongly disagreed with the retriever model.\n",
        "```\n",
        "\n",
        "*If the feedback model agrees with your retriever in all cases (if percentage is 0.00), there's little point in using the current setup for relevance feedback-based retrieval.*\n",
        "\n",
        "Then after a blazingly fast training, you'll get your parameters, something like:\n",
        "\n",
        "```\n",
        "Naive formula params: a=0.240764, b=1.348897, c=0.590883\n",
        "```\n",
        "\n",
        "These are the parameters customized to our documentation collection, `all-MiniLM-L6-v2` retriever and `mxbai-embed-large-v1` feedback model.\n",
        "\n",
        "Now we can use them for relevance feedback-based retrieval with any client of your choice."
      ],
      "metadata": {
        "id": "dXP4GdJGXTDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the Relevance Feedback Query\n",
        "\n",
        "Let's see how introducing the Relevance Feedback Query to a retrieval pipeline could work on this use case.\n",
        "\n",
        "Let's say you're powering documentation search with `all-MiniLM-L6-v2`, as it's very cheap and fast, but often it doesn't provide relevant enough results and you know there are more relevant parts of documentation than what `all-MiniLM-L6-v2` gets you.\n",
        "\n",
        "For example, let's look at this query:"
      ],
      "metadata": {
        "id": "-66d-td2XvY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"recommendations API how to use\""
      ],
      "metadata": {
        "id": "OCfSoQViXq9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vanilla Initial Retrieval\n",
        "\n",
        "We run simple semantic similarity search with our retriever, as we'd normally do in our search application."
      ],
      "metadata": {
        "id": "mlXKO_nYX7aW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_embedding = retriever.embed_query(query)\n",
        "CONTEXT_LIMIT = 3\n",
        "\n",
        "responses = client.query_points(\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    query=query_embedding,\n",
        "    with_payload=True,\n",
        "    with_vectors=True,\n",
        "    limit=CONTEXT_LIMIT,\n",
        "    using=RETRIEVER_VECTOR_NAME,\n",
        ").points"
      ],
      "metadata": {
        "id": "hMiRXlRREiGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s see what results we get:"
      ],
      "metadata": {
        "id": "NoiaogcdYFiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "responses_raw = [r.payload[PAYLOAD_KEY] for r in responses]\n",
        "\n",
        "print(\"\\n--- Initial Retrieval Results ---\")\n",
        "for i, text in enumerate(responses_raw, 1):\n",
        "    print(f\"  [{i}] {text[:200]}\")\n",
        "print()"
      ],
      "metadata": {
        "id": "AIJGG8TqEoht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll get something like:\n",
        "\n",
        "```\n",
        "--- Initial Retrieval Results ---\n",
        "  [1] Recommendation API\n",
        "  [2] recommendation API.\n",
        "  [3] So, even when the API is not called recommend, recommendation systems can also use this approach and adapt it for their specific use-cases.\n",
        "```\n",
        "\n",
        "We're seeing duplicates as our collection consists of chunks from our documentation website and some text is identical across different sections.\n",
        "\n",
        "Works, but perhaps there's something else in our collection that would answer the query better -- the retriever is just too weak to pick it up."
      ],
      "metadata": {
        "id": "_Aa6zm2JYKGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting Feedback on Initial Retrieval\n",
        "\n",
        "Now we get feedback from our `mxbai-embed-large-v1` feedback model on the top 3 results for the query “recommendations API how to use”.\n",
        "\n",
        "The feedback model rescores them according to its own judgement of semantic similarity. We only show it a small number of results (`CONTEXT_LIMIT` = 3) to keep the pipeline fast and cheap."
      ],
      "metadata": {
        "id": "ApkpoeOFYWNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feedback_model_scores = feedback.score(query, responses_raw)"
      ],
      "metadata": {
        "id": "L5dka7_hEyIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Relevance Feedback-based Retrieval\n",
        "\n",
        "Now we can strengthen our `all-MiniLM-L6-v2` retriever with the given feedback on 3 initial retrieved responses.\n",
        "\n",
        "We provide the feedback as a list of (example, score) pairs. Under the hood, this uses the mechanism of context pairs mining described in the [article](https://qdrant.tech/articles/search-feedback-loop/)."
      ],
      "metadata": {
        "id": "ZB4MILE1YeG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from qdrant_client import models\n",
        "\n",
        "responses_point_ids = [p.id for p in responses]\n",
        "#responses_vectors = [p.vector for p in responses]\n",
        "\n",
        "relevance_feedback_responses = client.query_points(\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    query=models.RelevanceFeedbackQuery(\n",
        "        relevance_feedback=models.RelevanceFeedbackInput(\n",
        "            target=query_embedding,\n",
        "            feedback=[\n",
        "                models.FeedbackItem(example=example, score=score)\n",
        "                for example, score in zip(responses_point_ids, feedback_model_scores) # or zip(responses_vectors, feedback_model_scores)\n",
        "            ],\n",
        "            strategy=models.NaiveFeedbackStrategy(\n",
        "                naive=models.NaiveFeedbackStrategyParams(**formula_params)\n",
        "            ),\n",
        "        )\n",
        "    ),\n",
        "    with_payload=True,\n",
        "    limit=3,\n",
        "    using=RETRIEVER_VECTOR_NAME,\n",
        ").points"
      ],
      "metadata": {
        "id": "2J8hMYTyFC-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s see what relevance feedback-based retrieval brings to the table:"
      ],
      "metadata": {
        "id": "6xdpE5e1YtxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "relevance_feedback_responses_raw = [r.payload[PAYLOAD_KEY] for r in relevance_feedback_responses]\n",
        "\n",
        "print(\"\\n--- Additional Results from Relevance Feedback-based Retrieval ---\")\n",
        "for i, text in enumerate(relevance_feedback_responses_raw, 1):\n",
        "    print(f\"  [{i}] {text[:200]}\")\n",
        "print()"
      ],
      "metadata": {
        "id": "WoxZc7aGFnJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It should return something like this:\n",
        "\n",
        "```\n",
        "--- Additional Results from Relevance Feedback-based Retrieval ---\n",
        "  [1] Recap of the old recommendations API\n",
        "  [2] The result of this API contains one array per recommendation requests.\n",
        "  [3] Deliver Better Recommendations with Qdrant's new API\n",
        "```"
      ],
      "metadata": {
        "id": "kbwQiBDTYxqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combining\n",
        "\n",
        "Now you can use Relevance Feedback Query results in different ways:\n",
        "\n",
        "- **Combine with initial retrieval**, for example rerank the union of both result sets with a feedback model.\n",
        "- **Use as your main results**, aka rely on the Relevance Feedback Query as the primary source of search results.\n",
        "\n",
        "Approach is tied to what you're passing to the `example` field in `FeedbackItem` (or analogues in other clients): raw retriever-produced embeddings or point IDs from your collection.\n",
        "\n",
        "*If you’re passing point IDs as examples, these points are automatically excluded from the results. To include them among other points, pass the raw vectors (see commented responses_vectors) instead of point IDs.*\n",
        "\n",
        "Empirically, the results on this query looked relevant. Yet what if you want to justify adding additional complexity to your pipeline? The framework we released also provides an evaluation block."
      ],
      "metadata": {
        "id": "pHKElCBrY5O8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "Analogously, we've generated a test set of queries."
      ],
      "metadata": {
        "id": "fA9YpvCEZQpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_queries = [\n",
        "    \"how to install qdrant locally\",\n",
        "    \"qdrant vector similarity search\",\n",
        "    \"qdrant HNSW filtering vs pre-filtering performance\",\n",
        "    \"qdrant payload filtering and indexing\",\n",
        "    \"qdrant docker setup\",\n",
        "    \"cosine similarity vs dot product qdrant\",\n",
        "    \"qdrant REST API authentication\",\n",
        "    \"batch upsert vectors qdrant\",\n",
        "    \"qdrant cloud deployment\",\n",
        "    \"scalar quantization qdrant\",\n",
        "    \"qdrant internals storage engine blog post\",\n",
        "    \"delete points by filter qdrant\",\n",
        "    \"qdrant named vectors\",\n",
        "    \"sparse vectors qdrant\",\n",
        "    \"qdrant scroll points pagination\",\n",
        "    \"what is a vector database\",\n",
        "    \"qdrant vs pinecone comparison\",\n",
        "    \"semantic search tutorial qdrant\",\n",
        "    \"RAG pipeline with qdrant\",\n",
        "    \"recommendations API how to use\",\n",
        "    \"qdrant use cases e-commerce sprase vectors\",\n",
        "    \"multimodal search images text qdrant\",\n",
        "    \"qdrant performance benchmarks\",\n",
        "    \"how vector search works explained\",\n",
        "    \"qdrant hybrid search blog\",\n",
        "    \"building chatbot with qdrant\",\n",
        "    \"qdrant new features release\",\n",
        "    \"neural search vs keyword search\",\n",
        "    \"qdrant fastembed integration tutorial\",\n",
        "    \"anomaly detection vector database\",\n",
        "    \"qdrant customer success story\",\n",
        "    \"LLM memory storage qdrant\",\n",
        "    \"qdrant product quantization explained\",\n",
        "    \"qdrant scalar vs product quantization comparison\",\n",
        "    \"qdrant binary quantization blog post\",\n",
        "    \"chunking strategies for RAG qdrant blog\",\n",
        "    \"qdrant JavaScript TypeScript SDK\",\n",
        "    \"real time vector search qdrant\",\n",
        "    \"qdrant rust performance internals\",\n",
        "    \"open source vector database comparison\",\n",
        "    \"qdrant snapshot backup restore\",\n",
        "    \"image search with qdrant clip\",\n",
        "    \"qdrant geo filtering use case\",\n",
        "    \"drug discovery vector search qdrant\",\n",
        "    \"qdrant multitenant architecture\",\n",
        "    \"on premise vs cloud vector database\",\n",
        "    \"qdrant web UI dashboard\",\n",
        "    \"qdrant funding news announcement\",\n",
        "    \"fine tuning embeddings qdrant\",\n",
        "    \"getting started vector search beginner\",\n",
        "]"
      ],
      "metadata": {
        "id": "6qLmjtITJxVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now let’s run our evaluation on this test set, and see how much gain we get, considering that our feedback model receives `CONTEXT_LIMIT` results per query to provide feedback on."
      ],
      "metadata": {
        "id": "2IS9pS3MZWtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from qdrant_relevance_feedback.evaluate import Evaluator\n",
        "\n",
        "N = 10  # as in metric@N\n",
        "\n",
        "evaluator = Evaluator(relevance_feedback=relevance_feedback)\n",
        "\n",
        "results = evaluator.evaluate_queries(\n",
        "    at_n=N,\n",
        "    formula_params=formula_params,\n",
        "    eval_queries=test_queries,\n",
        "    eval_context_limit=CONTEXT_LIMIT\n",
        ")"
      ],
      "metadata": {
        "id": "Mp_9DermZWcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll get output looking like this, providing us with *relative results relevance gain* and *discounted cumulative gain (DCG)* win rates at *N* (here, 10).\n",
        "\n",
        "```\n",
        "On the 2nd retrieval iteration, over this test set:\n",
        "  Relevance feedback retrieval surfaced 44 more relevant results (according to the feedback model)\n",
        "  Vanilla retrieval surfaced 39 more relevant results (according to the feedback model)\n",
        "\n",
        "Relative results relevance gain over this test set is: 12.82051282051282%\n",
        "\n",
        "DCG win rates (which approach ranks results better):\n",
        "  Vanilla retrieval:            38.0% wins\n",
        "  Relevance feedback retrieval:  48.0% wins\n",
        "  Ties:                          14.0%\n",
        "```\n",
        "\n",
        "**Relative results relevance gain:** over this test set of 50 generated queries, relevance feedback-based retrieval pulled 5 (44 - 39) more relevant documents from the collection, which the retriever was unable to “notice” – a 13% boost over vanilla retrieval.\n",
        "\n",
        "**DCG win rate:** on ~half of the queries (48%), relevance feedback-based retrieval ranks results within the evaluation window N better than the retriever would. In 14% of queries, both methods are identical, and in 38% relevance feedback confuses the retriever.\n",
        "\n",
        "A detailed explanation of the metrics is in the [article](https://qdrant.tech/articles/search-feedback-loop/), including accompanying visualizations."
      ],
      "metadata": {
        "id": "wWAIUpaKZjGH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "In this tutorial, we demonstrated how to use relevance feedback on text data to increase the relevance of results in your retrieval pipelines. Here's what to keep in mind:\n",
        "\n",
        "- **Adaptable to your data** -- works on different modalities, not only text. You can do the same with images, for example.\n",
        "- **Flexible model choice** -- you can use predefined retrievers and feedback models we support in Cloud Inference and FastEmbed, or define your own, anything, including LLMs and custom LTR models.\n",
        "- **Built-in evaluation** -- with our framework, you can also evaluate the potential gains of using the Relevance Feedback Query before putting it into production.\n",
        "\n",
        "If you'd like advice on Relevance Feedback Query usage or have ideas on how to enhance the method, reach out in our [Discord community](https://qdrant.to/discord)."
      ],
      "metadata": {
        "id": "cbZuS0DFZ1gS"
      }
    }
  ]
}