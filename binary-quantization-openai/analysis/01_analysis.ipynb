{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhance OpenAI Embeddings with Qdrant's Binary Quantization\n",
    "\n",
    "## Introduction\n",
    "\n",
    "OpenAI Ada-003 embeddings are a powerful tool for natural language processing (NLP). However, the size of the embeddings are a challenge, especially with real-time search and retrieval. In this article, we explore how you can use Qdrant's Binary Quantization to enhance the performance and efficiency of OpenAI embeddings.\n",
    "\n",
    "In this post, we discuss:\n",
    "\n",
    "- The significance of OpenAI embeddings and real-world challenges. \n",
    "- Qdrant's Binary Quantization, and how it can improve the performance of OpenAI embeddings\n",
    "- Results of an experiment that highlights improvements in search efficiency and accuracy\n",
    "- Implications of these findings for real-world applications\n",
    "- Best practices for leveraging Binary Quantization to enhance OpenAI embeddings\n",
    "\n",
    "## New OpenAI Embeddings: Performance and Changes\n",
    "As the technology of embedding models has advanced, demand has grown. Users are looking more for powerful and efficient text-embedding models. OpenAI's Ada-003 embeddings offer state-of-the-art performance on a wide range of NLP tasks, including those noted in [MTEB](https://huggingface.co/spaces/mteb/leaderboard) and [MIRACL](https://openai.com/blog/new-embedding-models-and-api-updates). \n",
    "\n",
    "### Multi-lingual Support\n",
    "OpenAI text-embedding-3-large is a multi-lingual model that can encode text in 100+ languages.\n",
    "\n",
    "As noted on MIRACL, text-embeeding-3-large is a significant improvement on text-embedding-ada-002. The average MTEB score has increased from 31.4% to 54.9%\n",
    "\n",
    "### Matryoshka Representation Learning\n",
    "The new OpenAI models have been trained with a novel approach called \"[Matryoshka Representation Learning](https://aniketrege.github.io/blog/2024/mrl/)\". Developers can set up embeddings of different sizes (number of dimensions). In this post, we use small and large variants. Developers can select embeddings which balances accuracy and size.\n",
    "\n",
    "Here, we show how the accuracy of binary quantization is quite good across different dimensions -- for both the models. \n",
    "\n",
    "## Enhanced Performance and Efficiency with Binary Quantization\n",
    "The efficiency gains from Binary Quantization are as follows: \n",
    "\n",
    "- Reduced storage footprint: It helps with large-scale datasets. It also saves on memory, and scales up to 30x at the same cost. \n",
    "- Enhanced speed of data retrieval: Smaller data sizes generally leads to faster searches. \n",
    "- Accelerated search process: It is based on simplified distance calculations between vectors to bitwise operations. This enables real-time querying even in extensive databases.\n",
    "\n",
    "![](Accuracy_Models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Setup: OpenAI Embeddings in Focus\n",
    "\n",
    "To identify Binary Quantization's impact on search efficiency and accuracy, we designed our experiment on OpenAI text-embedding models. These models, which capture nuanced linguistic features and semantic relationships, are the backbone of our analysis. We then delve deep into the potential enhancements offered by Qdrant's Binary Quantization feature.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We use 100K random samples from the [OpenAI 1M](https://huggingface.co/datasets/KShivendu/dbpedia-entities-openai-1M) dataset. We select 100 records at random from the dataset. We then use the embeddings of the queries to search for the nearest neighbors in the dataset. \n",
    "\n",
    "### Parameters: Oversampling, Rescoring, and Search Limits\n",
    "\n",
    "For each record, we run a parameter sweep over the number of oversampling, rescoring, and search limits. We can then understand the impact of these parameters on search accuracy and efficiency. Our experiment was designed to assess the impact of Binary Quantization under various conditions, based on the following parameters: \n",
    "\n",
    "- Oversampling\n",
    "- Rescoring\n",
    "- Search limits\n",
    "\n",
    "- **Oversampling**: By oversampling, we can limit the loss of information inherent in quantization. This also helps to preserve the semantic richness of your OpenAI embeddings. We experimented with different oversampling factors, and identified the impact on the accuracy and efficiency of search. Spoiler: higher oversampling factors tend to improve the accuracy of searches. However, they usually require more computational resources.\n",
    "\n",
    "- **Rescoring**: Rescoring refines the first results of an initial binary search. This process leverages the original high-dimensional vectors to refine the search results, **always** improving accuracy. We toggled rescoring on and off to measure effectiveness, when combined with Binary Quantization. We also measured the impact on search performance. \n",
    "\n",
    "- **Search Limits**: We specify the number of results from the search process. We experimented with various search limits to measure their impact the accuracy and efficiency. We explored the trade-offs between search depth and performance. The results provide insight for applications with different precision and speed requirements.\n",
    "\n",
    "Through this detailed setup, our experiment sought to shed light on the nuanced interplay between Binary Quantization and the high-quality embeddings produced by OpenAI's models. By meticulously adjusting and observing the outcomes under different conditions, we aimed to uncover actionable insights that could empower users to harness the full potential of Qdrant in combination with OpenAI's embeddings, regardless of their specific application needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results: Binary Quantization's Impact on OpenAI Embeddings\n",
    "\n",
    "To analyze the impact of rescoring (`True` or `False), we compared results across different model configurations and search limits. Rescoring sets up a more precise search, based on results from an initial query.\n",
    "\n",
    "### Rescoring\n",
    "\n",
    "![Graph that measures the impact of rescoring](Rescoring_Impact.png)\n",
    "\n",
    "Here are some key observations, which analyzes the impact of rescoring (`True` or `False`):\n",
    "\n",
    "1. **Significantly Improved Accuracy**:\n",
    "   - Across all models and dimension configurations, enabling rescoring (`True`) consistently results in higher accuracy scores compared to when rescoring is disabled (`False`).\n",
    "   - The improvement in accuracy is true across various search limits (10, 20, 50, 100).\n",
    "\n",
    "2. **Model and Dimension Specific Observations**:\n",
    "   - For the `text-embedding-3-large` model with 3072 dimensions, rescoring boosts the accuracy from an average of about 76-77% without rescoring to 97-99% with rescoring, depending on the search limit and oversampling rate.\n",
    "    - The accuracy improvement with increased oversampling is more pronounced when rescoring is enabled, indicating a better utilization of the additional binary codes in refining search results.\n",
    "   - With the `text-embedding-3-small` model at 512 dimensions, accuracy increases from around 53-55% without rescoring to 71-91% with rescoring, highlighting the significant impact of rescoring, especially at lower dimensions.\n",
    "   - For higher dimension models (such as text-embedding-3-large with 3072 dimensions), <NEED MORE INFO>\n",
    "In contrast, for lower dimension models (such as text-embedding-3-small with 512 dimensions), the incremental accuracy gains from increased oversampling levels are less significant, even with rescoring enabled. This suggests a diminishing return on accuracy improvement with higher oversampling in lower dimension spaces.\n",
    "\n",
    "3. **Influence of Search Limit**:\n",
    "   - The performance gain from rescoring seems to be relatively stable across different search limits, suggesting that rescoring consistently enhances accuracy regardless of the number of top results considered.\n",
    "\n",
    "In summary, enabling rescoring dramatically improves search accuracy across all tested configurations. It is crucial feature for applications where precision is paramount. The consistent performance boost provided by rescoring underscores its value in refining search results, particularly when working with complex, high-dimensional data like OpenAI embeddings. This enhancement is critical for applications that demand high accuracy, such as semantic search, content discovery, and recommendation systems, where the quality of search results directly impacts user experience and satisfaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_combinations = [\n",
    "    {\n",
    "        \"model_name\": \"text-embedding-3-large\",\n",
    "        \"dimensions\": 3072,\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"text-embedding-3-large\",\n",
    "        \"dimensions\": 1024,\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"text-embedding-3-large\",\n",
    "        \"dimensions\": 1536,\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"text-embedding-3-small\",\n",
    "        \"dimensions\": 512,\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"text-embedding-3-small\",\n",
    "        \"dimensions\": 1024,\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"text-embedding-3-small\",\n",
    "        \"dimensions\": 1536,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for combination in dataset_combinations:\n",
    "    model_name = combination[\"model_name\"]\n",
    "    dimensions = combination[\"dimensions\"]\n",
    "    print(f\"Model: {model_name}, dimensions: {dimensions}\")\n",
    "    results = pd.read_json(f\"../results/results-{model_name}-{dimensions}.json\", lines=True)\n",
    "    average_accuracy = results[results[\"limit\"] != 1]\n",
    "    average_accuracy = average_accuracy[average_accuracy[\"limit\"] != 5]\n",
    "    average_accuracy = average_accuracy.groupby([\"oversampling\", \"rescore\", \"limit\"])[\n",
    "        \"accuracy\"\n",
    "    ].mean()\n",
    "    average_accuracy = average_accuracy.reset_index()\n",
    "    acc = average_accuracy.pivot(\n",
    "        index=\"limit\", columns=[\"oversampling\", \"rescore\"], values=\"accuracy\"\n",
    "    )\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of Oversampling\n",
    "\n",
    "<EXPLAIN THIS GRAPH>\n",
    "![](Oversampling_Impact.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leveraging Binary Quantization: Best Practices\n",
    "We recommend the following best practices for leveraging Binary Quantization to enhance OpenAI embeddings:\n",
    "\n",
    "1. Embedding Model: Use the text-embedding-3-large from MTEB. It is most accurate among those tested.\n",
    "2. Dimensions: Use the highest dimension available for the model, to maximize accuracy. The results are true for English and other languages.\n",
    "3. Oversampling: Use an oversampling factor of 3 for the best balance between accuracy and efficiency. This factor is suitable for a wide range of applications.\n",
    "4. Rescoring: Enable rescoring to improve the accuracy of search results.\n",
    "5. RAM: Store the full vectors and payload on disk. Limit what you load from memory to the binary quantization index. This helps reduce the memory footprint and improve the overall efficiency of the system. The incremental latency from the disk read is negligible compared to the latency savings from the binary scoring in Qdrant, which uses SIMD instructions where possible.\n",
    "\n",
    "## Conclusion\n",
    "TBD\n",
    "\n",
    "## Call to Action\n",
    "TBD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
